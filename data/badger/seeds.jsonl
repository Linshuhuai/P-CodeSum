{"repo": "dgraph-io/badger", "path": "dir_unix.go", "func_name": "acquireDirectoryLock", "original_string": "func acquireDirectoryLock(dirPath string, pidFileName string, readOnly bool) (*directoryLockGuard, error) {\n\t// Convert to absolute path so that Release still works even if we do an unbalanced\n\t// chdir in the meantime.\n\tabsPidFilePath, err := filepath.Abs(filepath.Join(dirPath, pidFileName))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"cannot get absolute path for pid lock file\")\n\t}\n\tf, err := os.Open(dirPath)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"cannot open directory %q\", dirPath)\n\t}\n\topts := unix.LOCK_EX | unix.LOCK_NB\n\tif readOnly {\n\t\topts = unix.LOCK_SH | unix.LOCK_NB\n\t}\n\n\terr = unix.Flock(int(f.Fd()), opts)\n\tif err != nil {\n\t\tf.Close()\n\t\treturn nil, errors.Wrapf(err,\n\t\t\t\"Cannot acquire directory lock on %q.  Another process is using this Badger database.\",\n\t\t\tdirPath)\n\t}\n\n\tif !readOnly {\n\t\t// Yes, we happily overwrite a pre-existing pid file.  We're the\n\t\t// only read-write badger process using this directory.\n\t\terr = ioutil.WriteFile(absPidFilePath, []byte(fmt.Sprintf(\"%d\\n\", os.Getpid())), 0666)\n\t\tif err != nil {\n\t\t\tf.Close()\n\t\t\treturn nil, errors.Wrapf(err,\n\t\t\t\t\"Cannot write pid file %q\", absPidFilePath)\n\t\t}\n\t}\n\treturn &directoryLockGuard{f, absPidFilePath, readOnly}, nil\n}", "language": "go", "code": "func acquireDirectoryLock(dirPath string, pidFileName string, readOnly bool) (*directoryLockGuard, error) {\n\t// Convert to absolute path so that Release still works even if we do an unbalanced\n\t// chdir in the meantime.\n\tabsPidFilePath, err := filepath.Abs(filepath.Join(dirPath, pidFileName))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"cannot get absolute path for pid lock file\")\n\t}\n\tf, err := os.Open(dirPath)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"cannot open directory %q\", dirPath)\n\t}\n\topts := unix.LOCK_EX | unix.LOCK_NB\n\tif readOnly {\n\t\topts = unix.LOCK_SH | unix.LOCK_NB\n\t}\n\n\terr = unix.Flock(int(f.Fd()), opts)\n\tif err != nil {\n\t\tf.Close()\n\t\treturn nil, errors.Wrapf(err,\n\t\t\t\"Cannot acquire directory lock on %q.  Another process is using this Badger database.\",\n\t\t\tdirPath)\n\t}\n\n\tif !readOnly {\n\t\t// Yes, we happily overwrite a pre-existing pid file.  We're the\n\t\t// only read-write badger process using this directory.\n\t\terr = ioutil.WriteFile(absPidFilePath, []byte(fmt.Sprintf(\"%d\\n\", os.Getpid())), 0666)\n\t\tif err != nil {\n\t\t\tf.Close()\n\t\t\treturn nil, errors.Wrapf(err,\n\t\t\t\t\"Cannot write pid file %q\", absPidFilePath)\n\t\t}\n\t}\n\treturn &directoryLockGuard{f, absPidFilePath, readOnly}, nil\n}", "code_tokens": ["func", "acquireDirectoryLock", "(", "dirPath", "string", ",", "pidFileName", "string", ",", "readOnly", "bool", ")", "(", "*", "directoryLockGuard", ",", "error", ")", "{", "// Convert to absolute path so that Release still works even if we do an unbalanced", "// chdir in the meantime.", "absPidFilePath", ",", "err", ":=", "filepath", ".", "Abs", "(", "filepath", ".", "Join", "(", "dirPath", ",", "pidFileName", ")", ")", "\n", "if", "err", "!=", "nil", "{", "return", "nil", ",", "errors", ".", "Wrap", "(", "err", ",", "\"", "\"", ")", "\n", "}", "\n", "f", ",", "err", ":=", "os", ".", "Open", "(", "dirPath", ")", "\n", "if", "err", "!=", "nil", "{", "return", "nil", ",", "errors", ".", "Wrapf", "(", "err", ",", "\"", "\"", ",", "dirPath", ")", "\n", "}", "\n", "opts", ":=", "unix", ".", "LOCK_EX", "|", "unix", ".", "LOCK_NB", "\n", "if", "readOnly", "{", "opts", "=", "unix", ".", "LOCK_SH", "|", "unix", ".", "LOCK_NB", "\n", "}", "\n\n", "err", "=", "unix", ".", "Flock", "(", "int", "(", "f", ".", "Fd", "(", ")", ")", ",", "opts", ")", "\n", "if", "err", "!=", "nil", "{", "f", ".", "Close", "(", ")", "\n", "return", "nil", ",", "errors", ".", "Wrapf", "(", "err", ",", "\"", "\"", ",", "dirPath", ")", "\n", "}", "\n\n", "if", "!", "readOnly", "{", "// Yes, we happily overwrite a pre-existing pid file.  We're the", "// only read-write badger process using this directory.", "err", "=", "ioutil", ".", "WriteFile", "(", "absPidFilePath", ",", "[", "]", "byte", "(", "fmt", ".", "Sprintf", "(", "\"", "\\n", "\"", ",", "os", ".", "Getpid", "(", ")", ")", ")", ",", "0666", ")", "\n", "if", "err", "!=", "nil", "{", "f", ".", "Close", "(", ")", "\n", "return", "nil", ",", "errors", ".", "Wrapf", "(", "err", ",", "\"", "\"", ",", "absPidFilePath", ")", "\n", "}", "\n", "}", "\n", "return", "&", "directoryLockGuard", "{", "f", ",", "absPidFilePath", ",", "readOnly", "}", ",", "nil", "\n", "}"], "docstring": "// acquireDirectoryLock gets a lock on the directory (using flock). If\n// this is not read-only, it will also write our pid to\n// dirPath/pidFileName for convenience.", "docstring_tokens": ["acquireDirectoryLock", "gets", "a", "lock", "on", "the", "directory", "(", "using", "flock", ")", ".", "If", "this", "is", "not", "read", "-", "only", "it", "will", "also", "write", "our", "pid", "to", "dirPath", "/", "pidFileName", "for", "convenience", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/dir_unix.go#L45-L80", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "y/iterator.go", "func_name": "EncodedSize", "original_string": "func (v *ValueStruct) EncodedSize() uint16 {\n\tsz := len(v.Value) + 2 // meta, usermeta.\n\tif v.ExpiresAt == 0 {\n\t\treturn uint16(sz + 1)\n\t}\n\n\tenc := sizeVarint(v.ExpiresAt)\n\treturn uint16(sz + enc)\n}", "language": "go", "code": "func (v *ValueStruct) EncodedSize() uint16 {\n\tsz := len(v.Value) + 2 // meta, usermeta.\n\tif v.ExpiresAt == 0 {\n\t\treturn uint16(sz + 1)\n\t}\n\n\tenc := sizeVarint(v.ExpiresAt)\n\treturn uint16(sz + enc)\n}", "code_tokens": ["func", "(", "v", "*", "ValueStruct", ")", "EncodedSize", "(", ")", "uint16", "{", "sz", ":=", "len", "(", "v", ".", "Value", ")", "+", "2", "// meta, usermeta.", "\n", "if", "v", ".", "ExpiresAt", "==", "0", "{", "return", "uint16", "(", "sz", "+", "1", ")", "\n", "}", "\n\n", "enc", ":=", "sizeVarint", "(", "v", ".", "ExpiresAt", ")", "\n", "return", "uint16", "(", "sz", "+", "enc", ")", "\n", "}"], "docstring": "// EncodedSize is the size of the ValueStruct when encoded", "docstring_tokens": ["EncodedSize", "is", "the", "size", "of", "the", "ValueStruct", "when", "encoded"], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/y/iterator.go#L50-L58", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "y/iterator.go", "func_name": "Decode", "original_string": "func (v *ValueStruct) Decode(b []byte) {\n\tv.Meta = b[0]\n\tv.UserMeta = b[1]\n\tvar sz int\n\tv.ExpiresAt, sz = binary.Uvarint(b[2:])\n\tv.Value = b[2+sz:]\n}", "language": "go", "code": "func (v *ValueStruct) Decode(b []byte) {\n\tv.Meta = b[0]\n\tv.UserMeta = b[1]\n\tvar sz int\n\tv.ExpiresAt, sz = binary.Uvarint(b[2:])\n\tv.Value = b[2+sz:]\n}", "code_tokens": ["func", "(", "v", "*", "ValueStruct", ")", "Decode", "(", "b", "[", "]", "byte", ")", "{", "v", ".", "Meta", "=", "b", "[", "0", "]", "\n", "v", ".", "UserMeta", "=", "b", "[", "1", "]", "\n", "var", "sz", "int", "\n", "v", ".", "ExpiresAt", ",", "sz", "=", "binary", ".", "Uvarint", "(", "b", "[", "2", ":", "]", ")", "\n", "v", ".", "Value", "=", "b", "[", "2", "+", "sz", ":", "]", "\n", "}"], "docstring": "// Decode uses the length of the slice to infer the length of the Value field.", "docstring_tokens": ["Decode", "uses", "the", "length", "of", "the", "slice", "to", "infer", "the", "length", "of", "the", "Value", "field", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/y/iterator.go#L61-L67", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "structs.go", "func_name": "Encode", "original_string": "func (p valuePointer) Encode(b []byte) []byte {\n\tbinary.BigEndian.PutUint32(b[:4], p.Fid)\n\tbinary.BigEndian.PutUint32(b[4:8], p.Len)\n\tbinary.BigEndian.PutUint32(b[8:12], p.Offset)\n\treturn b[:vptrSize]\n}", "language": "go", "code": "func (p valuePointer) Encode(b []byte) []byte {\n\tbinary.BigEndian.PutUint32(b[:4], p.Fid)\n\tbinary.BigEndian.PutUint32(b[4:8], p.Len)\n\tbinary.BigEndian.PutUint32(b[8:12], p.Offset)\n\treturn b[:vptrSize]\n}", "code_tokens": ["func", "(", "p", "valuePointer", ")", "Encode", "(", "b", "[", "]", "byte", ")", "[", "]", "byte", "{", "binary", ".", "BigEndian", ".", "PutUint32", "(", "b", "[", ":", "4", "]", ",", "p", ".", "Fid", ")", "\n", "binary", ".", "BigEndian", ".", "PutUint32", "(", "b", "[", "4", ":", "8", "]", ",", "p", ".", "Len", ")", "\n", "binary", ".", "BigEndian", ".", "PutUint32", "(", "b", "[", "8", ":", "12", "]", ",", "p", ".", "Offset", ")", "\n", "return", "b", "[", ":", "vptrSize", "]", "\n", "}"], "docstring": "// Encode encodes Pointer into byte buffer.", "docstring_tokens": ["Encode", "encodes", "Pointer", "into", "byte", "buffer", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/structs.go#L35-L40", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "structs.go", "func_name": "Decode", "original_string": "func (h *header) Decode(buf []byte) {\n\th.klen = binary.BigEndian.Uint32(buf[0:4])\n\th.vlen = binary.BigEndian.Uint32(buf[4:8])\n\th.expiresAt = binary.BigEndian.Uint64(buf[8:16])\n\th.meta = buf[16]\n\th.userMeta = buf[17]\n}", "language": "go", "code": "func (h *header) Decode(buf []byte) {\n\th.klen = binary.BigEndian.Uint32(buf[0:4])\n\th.vlen = binary.BigEndian.Uint32(buf[4:8])\n\th.expiresAt = binary.BigEndian.Uint64(buf[8:16])\n\th.meta = buf[16]\n\th.userMeta = buf[17]\n}", "code_tokens": ["func", "(", "h", "*", "header", ")", "Decode", "(", "buf", "[", "]", "byte", ")", "{", "h", ".", "klen", "=", "binary", ".", "BigEndian", ".", "Uint32", "(", "buf", "[", "0", ":", "4", "]", ")", "\n", "h", ".", "vlen", "=", "binary", ".", "BigEndian", ".", "Uint32", "(", "buf", "[", "4", ":", "8", "]", ")", "\n", "h", ".", "expiresAt", "=", "binary", ".", "BigEndian", ".", "Uint64", "(", "buf", "[", "8", ":", "16", "]", ")", "\n", "h", ".", "meta", "=", "buf", "[", "16", "]", "\n", "h", ".", "userMeta", "=", "buf", "[", "17", "]", "\n", "}"], "docstring": "// Decodes h from buf.", "docstring_tokens": ["Decodes", "h", "from", "buf", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/structs.go#L71-L77", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "batch.go", "func_name": "NewWriteBatch", "original_string": "func (db *DB) NewWriteBatch() *WriteBatch {\n\ttxn := db.newTransaction(true, true)\n\t// If we let it stay at zero, compactions would not allow older key versions to be deleted,\n\t// because the read timestamps of pending txns, would be zero. Therefore, we set it to the\n\t// maximum read timestamp that's done. This allows compactions to discard key versions below\n\t// this read timestamp, while also not blocking on pending txns to finish before starting this\n\t// one.\n\ttxn.readTs = db.orc.readMark.DoneUntil()\n\treturn &WriteBatch{db: db, txn: txn}\n}", "language": "go", "code": "func (db *DB) NewWriteBatch() *WriteBatch {\n\ttxn := db.newTransaction(true, true)\n\t// If we let it stay at zero, compactions would not allow older key versions to be deleted,\n\t// because the read timestamps of pending txns, would be zero. Therefore, we set it to the\n\t// maximum read timestamp that's done. This allows compactions to discard key versions below\n\t// this read timestamp, while also not blocking on pending txns to finish before starting this\n\t// one.\n\ttxn.readTs = db.orc.readMark.DoneUntil()\n\treturn &WriteBatch{db: db, txn: txn}\n}", "code_tokens": ["func", "(", "db", "*", "DB", ")", "NewWriteBatch", "(", ")", "*", "WriteBatch", "{", "txn", ":=", "db", ".", "newTransaction", "(", "true", ",", "true", ")", "\n", "// If we let it stay at zero, compactions would not allow older key versions to be deleted,", "// because the read timestamps of pending txns, would be zero. Therefore, we set it to the", "// maximum read timestamp that's done. This allows compactions to discard key versions below", "// this read timestamp, while also not blocking on pending txns to finish before starting this", "// one.", "txn", ".", "readTs", "=", "db", ".", "orc", ".", "readMark", ".", "DoneUntil", "(", ")", "\n", "return", "&", "WriteBatch", "{", "db", ":", "db", ",", "txn", ":", "txn", "}", "\n", "}"], "docstring": "// NewWriteBatch creates a new WriteBatch. This provides a way to conveniently do a lot of writes,\n// batching them up as tightly as possible in a single transaction and using callbacks to avoid\n// waiting for them to commit, thus achieving good performance. This API hides away the logic of\n// creating and committing transactions. Due to the nature of SSI guaratees provided by Badger,\n// blind writes can never encounter transaction conflicts (ErrConflict).", "docstring_tokens": ["NewWriteBatch", "creates", "a", "new", "WriteBatch", ".", "This", "provides", "a", "way", "to", "conveniently", "do", "a", "lot", "of", "writes", "batching", "them", "up", "as", "tightly", "as", "possible", "in", "a", "single", "transaction", "and", "using", "callbacks", "to", "avoid", "waiting", "for", "them", "to", "commit", "thus", "achieving", "good", "performance", ".", "This", "API", "hides", "away", "the", "logic", "of", "creating", "and", "committing", "transactions", ".", "Due", "to", "the", "nature", "of", "SSI", "guaratees", "provided", "by", "Badger", "blind", "writes", "can", "never", "encounter", "transaction", "conflicts", "(", "ErrConflict", ")", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/batch.go#L38-L47", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "batch.go", "func_name": "SetEntry", "original_string": "func (wb *WriteBatch) SetEntry(e *Entry) error {\n\twb.Lock()\n\tdefer wb.Unlock()\n\n\tif err := wb.txn.SetEntry(e); err != ErrTxnTooBig {\n\t\treturn err\n\t}\n\t// Txn has reached it's zenith. Commit now.\n\tif cerr := wb.commit(); cerr != nil {\n\t\treturn cerr\n\t}\n\t// This time the error must not be ErrTxnTooBig, otherwise, we make the\n\t// error permanent.\n\tif err := wb.txn.SetEntry(e); err != nil {\n\t\twb.err = err\n\t\treturn err\n\t}\n\treturn nil\n}", "language": "go", "code": "func (wb *WriteBatch) SetEntry(e *Entry) error {\n\twb.Lock()\n\tdefer wb.Unlock()\n\n\tif err := wb.txn.SetEntry(e); err != ErrTxnTooBig {\n\t\treturn err\n\t}\n\t// Txn has reached it's zenith. Commit now.\n\tif cerr := wb.commit(); cerr != nil {\n\t\treturn cerr\n\t}\n\t// This time the error must not be ErrTxnTooBig, otherwise, we make the\n\t// error permanent.\n\tif err := wb.txn.SetEntry(e); err != nil {\n\t\twb.err = err\n\t\treturn err\n\t}\n\treturn nil\n}", "code_tokens": ["func", "(", "wb", "*", "WriteBatch", ")", "SetEntry", "(", "e", "*", "Entry", ")", "error", "{", "wb", ".", "Lock", "(", ")", "\n", "defer", "wb", ".", "Unlock", "(", ")", "\n\n", "if", "err", ":=", "wb", ".", "txn", ".", "SetEntry", "(", "e", ")", ";", "err", "!=", "ErrTxnTooBig", "{", "return", "err", "\n", "}", "\n", "// Txn has reached it's zenith. Commit now.", "if", "cerr", ":=", "wb", ".", "commit", "(", ")", ";", "cerr", "!=", "nil", "{", "return", "cerr", "\n", "}", "\n", "// This time the error must not be ErrTxnTooBig, otherwise, we make the", "// error permanent.", "if", "err", ":=", "wb", ".", "txn", ".", "SetEntry", "(", "e", ")", ";", "err", "!=", "nil", "{", "wb", ".", "err", "=", "err", "\n", "return", "err", "\n", "}", "\n", "return", "nil", "\n", "}"], "docstring": "// SetEntry is the equivalent of Txn.SetEntry.", "docstring_tokens": ["SetEntry", "is", "the", "equivalent", "of", "Txn", ".", "SetEntry", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/batch.go#L77-L95", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "batch.go", "func_name": "Set", "original_string": "func (wb *WriteBatch) Set(k, v []byte, meta byte) error {\n\te := &Entry{Key: k, Value: v, UserMeta: meta}\n\treturn wb.SetEntry(e)\n}", "language": "go", "code": "func (wb *WriteBatch) Set(k, v []byte, meta byte) error {\n\te := &Entry{Key: k, Value: v, UserMeta: meta}\n\treturn wb.SetEntry(e)\n}", "code_tokens": ["func", "(", "wb", "*", "WriteBatch", ")", "Set", "(", "k", ",", "v", "[", "]", "byte", ",", "meta", "byte", ")", "error", "{", "e", ":=", "&", "Entry", "{", "Key", ":", "k", ",", "Value", ":", "v", ",", "UserMeta", ":", "meta", "}", "\n", "return", "wb", ".", "SetEntry", "(", "e", ")", "\n", "}"], "docstring": "// Set is equivalent of Txn.SetWithMeta.", "docstring_tokens": ["Set", "is", "equivalent", "of", "Txn", ".", "SetWithMeta", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/batch.go#L98-L101", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "db.go", "func_name": "Open", "original_string": "func Open(opt Options) (db *DB, err error) {\n\topt.maxBatchSize = (15 * opt.MaxTableSize) / 100\n\topt.maxBatchCount = opt.maxBatchSize / int64(skl.MaxNodeSize)\n\n\tif opt.ValueThreshold > math.MaxUint16-16 {\n\t\treturn nil, ErrValueThreshold\n\t}\n\n\tif opt.ReadOnly {\n\t\t// Can't truncate if the DB is read only.\n\t\topt.Truncate = false\n\t\t// Do not perform compaction in read only mode.\n\t\topt.CompactL0OnClose = false\n\t}\n\n\tfor _, path := range []string{opt.Dir, opt.ValueDir} {\n\t\tdirExists, err := exists(path)\n\t\tif err != nil {\n\t\t\treturn nil, y.Wrapf(err, \"Invalid Dir: %q\", path)\n\t\t}\n\t\tif !dirExists {\n\t\t\tif opt.ReadOnly {\n\t\t\t\treturn nil, y.Wrapf(err, \"Cannot find Dir for read-only open: %q\", path)\n\t\t\t}\n\t\t\t// Try to create the directory\n\t\t\terr = os.Mkdir(path, 0700)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, y.Wrapf(err, \"Error Creating Dir: %q\", path)\n\t\t\t}\n\t\t}\n\t}\n\tabsDir, err := filepath.Abs(opt.Dir)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tabsValueDir, err := filepath.Abs(opt.ValueDir)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvar dirLockGuard, valueDirLockGuard *directoryLockGuard\n\tdirLockGuard, err = acquireDirectoryLock(opt.Dir, lockFile, opt.ReadOnly)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func() {\n\t\tif dirLockGuard != nil {\n\t\t\t_ = dirLockGuard.release()\n\t\t}\n\t}()\n\tif absValueDir != absDir {\n\t\tvalueDirLockGuard, err = acquireDirectoryLock(opt.ValueDir, lockFile, opt.ReadOnly)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tdefer func() {\n\t\t\tif valueDirLockGuard != nil {\n\t\t\t\t_ = valueDirLockGuard.release()\n\t\t\t}\n\t\t}()\n\t}\n\tif !(opt.ValueLogFileSize <= 2<<30 && opt.ValueLogFileSize >= 1<<20) {\n\t\treturn nil, ErrValueLogSize\n\t}\n\tif !(opt.ValueLogLoadingMode == options.FileIO ||\n\t\topt.ValueLogLoadingMode == options.MemoryMap) {\n\t\treturn nil, ErrInvalidLoadingMode\n\t}\n\tmanifestFile, manifest, err := openOrCreateManifestFile(opt.Dir, opt.ReadOnly)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func() {\n\t\tif manifestFile != nil {\n\t\t\t_ = manifestFile.close()\n\t\t}\n\t}()\n\n\tdb = &DB{\n\t\timm:           make([]*skl.Skiplist, 0, opt.NumMemtables),\n\t\tflushChan:     make(chan flushTask, opt.NumMemtables),\n\t\twriteCh:       make(chan *request, kvWriteChCapacity),\n\t\topt:           opt,\n\t\tmanifest:      manifestFile,\n\t\telog:          trace.NewEventLog(\"Badger\", \"DB\"),\n\t\tdirLockGuard:  dirLockGuard,\n\t\tvalueDirGuard: valueDirLockGuard,\n\t\torc:           newOracle(opt),\n\t}\n\n\t// Calculate initial size.\n\tdb.calculateSize()\n\tdb.closers.updateSize = y.NewCloser(1)\n\tgo db.updateSize(db.closers.updateSize)\n\tdb.mt = skl.NewSkiplist(arenaSize(opt))\n\n\t// newLevelsController potentially loads files in directory.\n\tif db.lc, err = newLevelsController(db, &manifest); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif !opt.ReadOnly {\n\t\tdb.closers.compactors = y.NewCloser(1)\n\t\tdb.lc.startCompact(db.closers.compactors)\n\n\t\tdb.closers.memtable = y.NewCloser(1)\n\t\tgo db.flushMemtable(db.closers.memtable) // Need levels controller to be up.\n\t}\n\n\theadKey := y.KeyWithTs(head, math.MaxUint64)\n\t// Need to pass with timestamp, lsm get removes the last 8 bytes and compares key\n\tvs, err := db.get(headKey)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Retrieving head\")\n\t}\n\tdb.orc.nextTxnTs = vs.Version\n\tvar vptr valuePointer\n\tif len(vs.Value) > 0 {\n\t\tvptr.Decode(vs.Value)\n\t}\n\n\treplayCloser := y.NewCloser(1)\n\tgo db.doWrites(replayCloser)\n\n\tif err = db.vlog.open(db, vptr, db.replayFunction()); err != nil {\n\t\treturn db, err\n\t}\n\treplayCloser.SignalAndWait() // Wait for replay to be applied first.\n\n\t// Let's advance nextTxnTs to one more than whatever we observed via\n\t// replaying the logs.\n\tdb.orc.txnMark.Done(db.orc.nextTxnTs)\n\t// In normal mode, we must update readMark so older versions of keys can be removed during\n\t// compaction when run in offline mode via the flatten tool.\n\tdb.orc.readMark.Done(db.orc.nextTxnTs)\n\tdb.orc.nextTxnTs++\n\n\tdb.writeCh = make(chan *request, kvWriteChCapacity)\n\tdb.closers.writes = y.NewCloser(1)\n\tgo db.doWrites(db.closers.writes)\n\n\tdb.closers.valueGC = y.NewCloser(1)\n\tgo db.vlog.waitOnGC(db.closers.valueGC)\n\n\tvalueDirLockGuard = nil\n\tdirLockGuard = nil\n\tmanifestFile = nil\n\treturn db, nil\n}", "language": "go", "code": "func Open(opt Options) (db *DB, err error) {\n\topt.maxBatchSize = (15 * opt.MaxTableSize) / 100\n\topt.maxBatchCount = opt.maxBatchSize / int64(skl.MaxNodeSize)\n\n\tif opt.ValueThreshold > math.MaxUint16-16 {\n\t\treturn nil, ErrValueThreshold\n\t}\n\n\tif opt.ReadOnly {\n\t\t// Can't truncate if the DB is read only.\n\t\topt.Truncate = false\n\t\t// Do not perform compaction in read only mode.\n\t\topt.CompactL0OnClose = false\n\t}\n\n\tfor _, path := range []string{opt.Dir, opt.ValueDir} {\n\t\tdirExists, err := exists(path)\n\t\tif err != nil {\n\t\t\treturn nil, y.Wrapf(err, \"Invalid Dir: %q\", path)\n\t\t}\n\t\tif !dirExists {\n\t\t\tif opt.ReadOnly {\n\t\t\t\treturn nil, y.Wrapf(err, \"Cannot find Dir for read-only open: %q\", path)\n\t\t\t}\n\t\t\t// Try to create the directory\n\t\t\terr = os.Mkdir(path, 0700)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, y.Wrapf(err, \"Error Creating Dir: %q\", path)\n\t\t\t}\n\t\t}\n\t}\n\tabsDir, err := filepath.Abs(opt.Dir)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tabsValueDir, err := filepath.Abs(opt.ValueDir)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvar dirLockGuard, valueDirLockGuard *directoryLockGuard\n\tdirLockGuard, err = acquireDirectoryLock(opt.Dir, lockFile, opt.ReadOnly)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func() {\n\t\tif dirLockGuard != nil {\n\t\t\t_ = dirLockGuard.release()\n\t\t}\n\t}()\n\tif absValueDir != absDir {\n\t\tvalueDirLockGuard, err = acquireDirectoryLock(opt.ValueDir, lockFile, opt.ReadOnly)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tdefer func() {\n\t\t\tif valueDirLockGuard != nil {\n\t\t\t\t_ = valueDirLockGuard.release()\n\t\t\t}\n\t\t}()\n\t}\n\tif !(opt.ValueLogFileSize <= 2<<30 && opt.ValueLogFileSize >= 1<<20) {\n\t\treturn nil, ErrValueLogSize\n\t}\n\tif !(opt.ValueLogLoadingMode == options.FileIO ||\n\t\topt.ValueLogLoadingMode == options.MemoryMap) {\n\t\treturn nil, ErrInvalidLoadingMode\n\t}\n\tmanifestFile, manifest, err := openOrCreateManifestFile(opt.Dir, opt.ReadOnly)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func() {\n\t\tif manifestFile != nil {\n\t\t\t_ = manifestFile.close()\n\t\t}\n\t}()\n\n\tdb = &DB{\n\t\timm:           make([]*skl.Skiplist, 0, opt.NumMemtables),\n\t\tflushChan:     make(chan flushTask, opt.NumMemtables),\n\t\twriteCh:       make(chan *request, kvWriteChCapacity),\n\t\topt:           opt,\n\t\tmanifest:      manifestFile,\n\t\telog:          trace.NewEventLog(\"Badger\", \"DB\"),\n\t\tdirLockGuard:  dirLockGuard,\n\t\tvalueDirGuard: valueDirLockGuard,\n\t\torc:           newOracle(opt),\n\t}\n\n\t// Calculate initial size.\n\tdb.calculateSize()\n\tdb.closers.updateSize = y.NewCloser(1)\n\tgo db.updateSize(db.closers.updateSize)\n\tdb.mt = skl.NewSkiplist(arenaSize(opt))\n\n\t// newLevelsController potentially loads files in directory.\n\tif db.lc, err = newLevelsController(db, &manifest); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif !opt.ReadOnly {\n\t\tdb.closers.compactors = y.NewCloser(1)\n\t\tdb.lc.startCompact(db.closers.compactors)\n\n\t\tdb.closers.memtable = y.NewCloser(1)\n\t\tgo db.flushMemtable(db.closers.memtable) // Need levels controller to be up.\n\t}\n\n\theadKey := y.KeyWithTs(head, math.MaxUint64)\n\t// Need to pass with timestamp, lsm get removes the last 8 bytes and compares key\n\tvs, err := db.get(headKey)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Retrieving head\")\n\t}\n\tdb.orc.nextTxnTs = vs.Version\n\tvar vptr valuePointer\n\tif len(vs.Value) > 0 {\n\t\tvptr.Decode(vs.Value)\n\t}\n\n\treplayCloser := y.NewCloser(1)\n\tgo db.doWrites(replayCloser)\n\n\tif err = db.vlog.open(db, vptr, db.replayFunction()); err != nil {\n\t\treturn db, err\n\t}\n\treplayCloser.SignalAndWait() // Wait for replay to be applied first.\n\n\t// Let's advance nextTxnTs to one more than whatever we observed via\n\t// replaying the logs.\n\tdb.orc.txnMark.Done(db.orc.nextTxnTs)\n\t// In normal mode, we must update readMark so older versions of keys can be removed during\n\t// compaction when run in offline mode via the flatten tool.\n\tdb.orc.readMark.Done(db.orc.nextTxnTs)\n\tdb.orc.nextTxnTs++\n\n\tdb.writeCh = make(chan *request, kvWriteChCapacity)\n\tdb.closers.writes = y.NewCloser(1)\n\tgo db.doWrites(db.closers.writes)\n\n\tdb.closers.valueGC = y.NewCloser(1)\n\tgo db.vlog.waitOnGC(db.closers.valueGC)\n\n\tvalueDirLockGuard = nil\n\tdirLockGuard = nil\n\tmanifestFile = nil\n\treturn db, nil\n}", "code_tokens": ["func", "Open", "(", "opt", "Options", ")", "(", "db", "*", "DB", ",", "err", "error", ")", "{", "opt", ".", "maxBatchSize", "=", "(", "15", "*", "opt", ".", "MaxTableSize", ")", "/", "100", "\n", "opt", ".", "maxBatchCount", "=", "opt", ".", "maxBatchSize", "/", "int64", "(", "skl", ".", "MaxNodeSize", ")", "\n\n", "if", "opt", ".", "ValueThreshold", ">", "math", ".", "MaxUint16", "-", "16", "{", "return", "nil", ",", "ErrValueThreshold", "\n", "}", "\n\n", "if", "opt", ".", "ReadOnly", "{", "// Can't truncate if the DB is read only.", "opt", ".", "Truncate", "=", "false", "\n", "// Do not perform compaction in read only mode.", "opt", ".", "CompactL0OnClose", "=", "false", "\n", "}", "\n\n", "for", "_", ",", "path", ":=", "range", "[", "]", "string", "{", "opt", ".", "Dir", ",", "opt", ".", "ValueDir", "}", "{", "dirExists", ",", "err", ":=", "exists", "(", "path", ")", "\n", "if", "err", "!=", "nil", "{", "return", "nil", ",", "y", ".", "Wrapf", "(", "err", ",", "\"", "\"", ",", "path", ")", "\n", "}", "\n", "if", "!", "dirExists", "{", "if", "opt", ".", "ReadOnly", "{", "return", "nil", ",", "y", ".", "Wrapf", "(", "err", ",", "\"", "\"", ",", "path", ")", "\n", "}", "\n", "// Try to create the directory", "err", "=", "os", ".", "Mkdir", "(", "path", ",", "0700", ")", "\n", "if", "err", "!=", "nil", "{", "return", "nil", ",", "y", ".", "Wrapf", "(", "err", ",", "\"", "\"", ",", "path", ")", "\n", "}", "\n", "}", "\n", "}", "\n", "absDir", ",", "err", ":=", "filepath", ".", "Abs", "(", "opt", ".", "Dir", ")", "\n", "if", "err", "!=", "nil", "{", "return", "nil", ",", "err", "\n", "}", "\n", "absValueDir", ",", "err", ":=", "filepath", ".", "Abs", "(", "opt", ".", "ValueDir", ")", "\n", "if", "err", "!=", "nil", "{", "return", "nil", ",", "err", "\n", "}", "\n", "var", "dirLockGuard", ",", "valueDirLockGuard", "*", "directoryLockGuard", "\n", "dirLockGuard", ",", "err", "=", "acquireDirectoryLock", "(", "opt", ".", "Dir", ",", "lockFile", ",", "opt", ".", "ReadOnly", ")", "\n", "if", "err", "!=", "nil", "{", "return", "nil", ",", "err", "\n", "}", "\n", "defer", "func", "(", ")", "{", "if", "dirLockGuard", "!=", "nil", "{", "_", "=", "dirLockGuard", ".", "release", "(", ")", "\n", "}", "\n", "}", "(", ")", "\n", "if", "absValueDir", "!=", "absDir", "{", "valueDirLockGuard", ",", "err", "=", "acquireDirectoryLock", "(", "opt", ".", "ValueDir", ",", "lockFile", ",", "opt", ".", "ReadOnly", ")", "\n", "if", "err", "!=", "nil", "{", "return", "nil", ",", "err", "\n", "}", "\n", "defer", "func", "(", ")", "{", "if", "valueDirLockGuard", "!=", "nil", "{", "_", "=", "valueDirLockGuard", ".", "release", "(", ")", "\n", "}", "\n", "}", "(", ")", "\n", "}", "\n", "if", "!", "(", "opt", ".", "ValueLogFileSize", "<=", "2", "<<", "30", "&&", "opt", ".", "ValueLogFileSize", ">=", "1", "<<", "20", ")", "{", "return", "nil", ",", "ErrValueLogSize", "\n", "}", "\n", "if", "!", "(", "opt", ".", "ValueLogLoadingMode", "==", "options", ".", "FileIO", "||", "opt", ".", "ValueLogLoadingMode", "==", "options", ".", "MemoryMap", ")", "{", "return", "nil", ",", "ErrInvalidLoadingMode", "\n", "}", "\n", "manifestFile", ",", "manifest", ",", "err", ":=", "openOrCreateManifestFile", "(", "opt", ".", "Dir", ",", "opt", ".", "ReadOnly", ")", "\n", "if", "err", "!=", "nil", "{", "return", "nil", ",", "err", "\n", "}", "\n", "defer", "func", "(", ")", "{", "if", "manifestFile", "!=", "nil", "{", "_", "=", "manifestFile", ".", "close", "(", ")", "\n", "}", "\n", "}", "(", ")", "\n\n", "db", "=", "&", "DB", "{", "imm", ":", "make", "(", "[", "]", "*", "skl", ".", "Skiplist", ",", "0", ",", "opt", ".", "NumMemtables", ")", ",", "flushChan", ":", "make", "(", "chan", "flushTask", ",", "opt", ".", "NumMemtables", ")", ",", "writeCh", ":", "make", "(", "chan", "*", "request", ",", "kvWriteChCapacity", ")", ",", "opt", ":", "opt", ",", "manifest", ":", "manifestFile", ",", "elog", ":", "trace", ".", "NewEventLog", "(", "\"", "\"", ",", "\"", "\"", ")", ",", "dirLockGuard", ":", "dirLockGuard", ",", "valueDirGuard", ":", "valueDirLockGuard", ",", "orc", ":", "newOracle", "(", "opt", ")", ",", "}", "\n\n", "// Calculate initial size.", "db", ".", "calculateSize", "(", ")", "\n", "db", ".", "closers", ".", "updateSize", "=", "y", ".", "NewCloser", "(", "1", ")", "\n", "go", "db", ".", "updateSize", "(", "db", ".", "closers", ".", "updateSize", ")", "\n", "db", ".", "mt", "=", "skl", ".", "NewSkiplist", "(", "arenaSize", "(", "opt", ")", ")", "\n\n", "// newLevelsController potentially loads files in directory.", "if", "db", ".", "lc", ",", "err", "=", "newLevelsController", "(", "db", ",", "&", "manifest", ")", ";", "err", "!=", "nil", "{", "return", "nil", ",", "err", "\n", "}", "\n\n", "if", "!", "opt", ".", "ReadOnly", "{", "db", ".", "closers", ".", "compactors", "=", "y", ".", "NewCloser", "(", "1", ")", "\n", "db", ".", "lc", ".", "startCompact", "(", "db", ".", "closers", ".", "compactors", ")", "\n\n", "db", ".", "closers", ".", "memtable", "=", "y", ".", "NewCloser", "(", "1", ")", "\n", "go", "db", ".", "flushMemtable", "(", "db", ".", "closers", ".", "memtable", ")", "// Need levels controller to be up.", "\n", "}", "\n\n", "headKey", ":=", "y", ".", "KeyWithTs", "(", "head", ",", "math", ".", "MaxUint64", ")", "\n", "// Need to pass with timestamp, lsm get removes the last 8 bytes and compares key", "vs", ",", "err", ":=", "db", ".", "get", "(", "headKey", ")", "\n", "if", "err", "!=", "nil", "{", "return", "nil", ",", "errors", ".", "Wrap", "(", "err", ",", "\"", "\"", ")", "\n", "}", "\n", "db", ".", "orc", ".", "nextTxnTs", "=", "vs", ".", "Version", "\n", "var", "vptr", "valuePointer", "\n", "if", "len", "(", "vs", ".", "Value", ")", ">", "0", "{", "vptr", ".", "Decode", "(", "vs", ".", "Value", ")", "\n", "}", "\n\n", "replayCloser", ":=", "y", ".", "NewCloser", "(", "1", ")", "\n", "go", "db", ".", "doWrites", "(", "replayCloser", ")", "\n\n", "if", "err", "=", "db", ".", "vlog", ".", "open", "(", "db", ",", "vptr", ",", "db", ".", "replayFunction", "(", ")", ")", ";", "err", "!=", "nil", "{", "return", "db", ",", "err", "\n", "}", "\n", "replayCloser", ".", "SignalAndWait", "(", ")", "// Wait for replay to be applied first.", "\n\n", "// Let's advance nextTxnTs to one more than whatever we observed via", "// replaying the logs.", "db", ".", "orc", ".", "txnMark", ".", "Done", "(", "db", ".", "orc", ".", "nextTxnTs", ")", "\n", "// In normal mode, we must update readMark so older versions of keys can be removed during", "// compaction when run in offline mode via the flatten tool.", "db", ".", "orc", ".", "readMark", ".", "Done", "(", "db", ".", "orc", ".", "nextTxnTs", ")", "\n", "db", ".", "orc", ".", "nextTxnTs", "++", "\n\n", "db", ".", "writeCh", "=", "make", "(", "chan", "*", "request", ",", "kvWriteChCapacity", ")", "\n", "db", ".", "closers", ".", "writes", "=", "y", ".", "NewCloser", "(", "1", ")", "\n", "go", "db", ".", "doWrites", "(", "db", ".", "closers", ".", "writes", ")", "\n\n", "db", ".", "closers", ".", "valueGC", "=", "y", ".", "NewCloser", "(", "1", ")", "\n", "go", "db", ".", "vlog", ".", "waitOnGC", "(", "db", ".", "closers", ".", "valueGC", ")", "\n\n", "valueDirLockGuard", "=", "nil", "\n", "dirLockGuard", "=", "nil", "\n", "manifestFile", "=", "nil", "\n", "return", "db", ",", "nil", "\n", "}"], "docstring": "// Open returns a new DB object.", "docstring_tokens": ["Open", "returns", "a", "new", "DB", "object", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/db.go#L178-L325", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "db.go", "func_name": "syncDir", "original_string": "func syncDir(dir string) error {\n\tf, err := openDir(dir)\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"While opening directory: %s.\", dir)\n\t}\n\terr = f.Sync()\n\tcloseErr := f.Close()\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"While syncing directory: %s.\", dir)\n\t}\n\treturn errors.Wrapf(closeErr, \"While closing directory: %s.\", dir)\n}", "language": "go", "code": "func syncDir(dir string) error {\n\tf, err := openDir(dir)\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"While opening directory: %s.\", dir)\n\t}\n\terr = f.Sync()\n\tcloseErr := f.Close()\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"While syncing directory: %s.\", dir)\n\t}\n\treturn errors.Wrapf(closeErr, \"While closing directory: %s.\", dir)\n}", "code_tokens": ["func", "syncDir", "(", "dir", "string", ")", "error", "{", "f", ",", "err", ":=", "openDir", "(", "dir", ")", "\n", "if", "err", "!=", "nil", "{", "return", "errors", ".", "Wrapf", "(", "err", ",", "\"", "\"", ",", "dir", ")", "\n", "}", "\n", "err", "=", "f", ".", "Sync", "(", ")", "\n", "closeErr", ":=", "f", ".", "Close", "(", ")", "\n", "if", "err", "!=", "nil", "{", "return", "errors", ".", "Wrapf", "(", "err", ",", "\"", "\"", ",", "dir", ")", "\n", "}", "\n", "return", "errors", ".", "Wrapf", "(", "closeErr", ",", "\"", "\"", ",", "dir", ")", "\n", "}"], "docstring": "// When you create or delete a file, you have to ensure the directory entry for the file is synced\n// in order to guarantee the file is visible (if the system crashes).  (See the man page for fsync,\n// or see https://github.com/coreos/etcd/issues/6368 for an example.)", "docstring_tokens": ["When", "you", "create", "or", "delete", "a", "file", "you", "have", "to", "ensure", "the", "directory", "entry", "for", "the", "file", "is", "synced", "in", "order", "to", "guarantee", "the", "file", "is", "visible", "(", "if", "the", "system", "crashes", ")", ".", "(", "See", "the", "man", "page", "for", "fsync", "or", "see", "https", ":", "//", "github", ".", "com", "/", "coreos", "/", "etcd", "/", "issues", "/", "6368", "for", "an", "example", ".", ")"], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/db.go#L442-L453", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "db.go", "func_name": "getMemTables", "original_string": "func (db *DB) getMemTables() ([]*skl.Skiplist, func()) {\n\tdb.RLock()\n\tdefer db.RUnlock()\n\n\ttables := make([]*skl.Skiplist, len(db.imm)+1)\n\n\t// Get mutable memtable.\n\ttables[0] = db.mt\n\ttables[0].IncrRef()\n\n\t// Get immutable memtables.\n\tlast := len(db.imm) - 1\n\tfor i := range db.imm {\n\t\ttables[i+1] = db.imm[last-i]\n\t\ttables[i+1].IncrRef()\n\t}\n\treturn tables, func() {\n\t\tfor _, tbl := range tables {\n\t\t\ttbl.DecrRef()\n\t\t}\n\t}\n}", "language": "go", "code": "func (db *DB) getMemTables() ([]*skl.Skiplist, func()) {\n\tdb.RLock()\n\tdefer db.RUnlock()\n\n\ttables := make([]*skl.Skiplist, len(db.imm)+1)\n\n\t// Get mutable memtable.\n\ttables[0] = db.mt\n\ttables[0].IncrRef()\n\n\t// Get immutable memtables.\n\tlast := len(db.imm) - 1\n\tfor i := range db.imm {\n\t\ttables[i+1] = db.imm[last-i]\n\t\ttables[i+1].IncrRef()\n\t}\n\treturn tables, func() {\n\t\tfor _, tbl := range tables {\n\t\t\ttbl.DecrRef()\n\t\t}\n\t}\n}", "code_tokens": ["func", "(", "db", "*", "DB", ")", "getMemTables", "(", ")", "(", "[", "]", "*", "skl", ".", "Skiplist", ",", "func", "(", ")", ")", "{", "db", ".", "RLock", "(", ")", "\n", "defer", "db", ".", "RUnlock", "(", ")", "\n\n", "tables", ":=", "make", "(", "[", "]", "*", "skl", ".", "Skiplist", ",", "len", "(", "db", ".", "imm", ")", "+", "1", ")", "\n\n", "// Get mutable memtable.", "tables", "[", "0", "]", "=", "db", ".", "mt", "\n", "tables", "[", "0", "]", ".", "IncrRef", "(", ")", "\n\n", "// Get immutable memtables.", "last", ":=", "len", "(", "db", ".", "imm", ")", "-", "1", "\n", "for", "i", ":=", "range", "db", ".", "imm", "{", "tables", "[", "i", "+", "1", "]", "=", "db", ".", "imm", "[", "last", "-", "i", "]", "\n", "tables", "[", "i", "+", "1", "]", ".", "IncrRef", "(", ")", "\n", "}", "\n", "return", "tables", ",", "func", "(", ")", "{", "for", "_", ",", "tbl", ":=", "range", "tables", "{", "tbl", ".", "DecrRef", "(", ")", "\n", "}", "\n", "}", "\n", "}"], "docstring": "// getMemtables returns the current memtables and get references.", "docstring_tokens": ["getMemtables", "returns", "the", "current", "memtables", "and", "get", "references", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/db.go#L456-L477", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "y/mmap_unix.go", "func_name": "Mmap", "original_string": "func Mmap(fd *os.File, writable bool, size int64) ([]byte, error) {\n\tmtype := unix.PROT_READ\n\tif writable {\n\t\tmtype |= unix.PROT_WRITE\n\t}\n\treturn unix.Mmap(int(fd.Fd()), 0, int(size), mtype, unix.MAP_SHARED)\n}", "language": "go", "code": "func Mmap(fd *os.File, writable bool, size int64) ([]byte, error) {\n\tmtype := unix.PROT_READ\n\tif writable {\n\t\tmtype |= unix.PROT_WRITE\n\t}\n\treturn unix.Mmap(int(fd.Fd()), 0, int(size), mtype, unix.MAP_SHARED)\n}", "code_tokens": ["func", "Mmap", "(", "fd", "*", "os", ".", "File", ",", "writable", "bool", ",", "size", "int64", ")", "(", "[", "]", "byte", ",", "error", ")", "{", "mtype", ":=", "unix", ".", "PROT_READ", "\n", "if", "writable", "{", "mtype", "|=", "unix", ".", "PROT_WRITE", "\n", "}", "\n", "return", "unix", ".", "Mmap", "(", "int", "(", "fd", ".", "Fd", "(", ")", ")", ",", "0", ",", "int", "(", "size", ")", ",", "mtype", ",", "unix", ".", "MAP_SHARED", ")", "\n", "}"], "docstring": "// Mmap uses the mmap system call to memory-map a file. If writable is true,\n// memory protection of the pages is set so that they may be written to as well.", "docstring_tokens": ["Mmap", "uses", "the", "mmap", "system", "call", "to", "memory", "-", "map", "a", "file", ".", "If", "writable", "is", "true", "memory", "protection", "of", "the", "pages", "is", "set", "so", "that", "they", "may", "be", "written", "to", "as", "well", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/y/mmap_unix.go#L31-L37", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "txn.go", "func_name": "setDiscardTs", "original_string": "func (o *oracle) setDiscardTs(ts uint64) {\n\to.Lock()\n\tdefer o.Unlock()\n\to.discardTs = ts\n}", "language": "go", "code": "func (o *oracle) setDiscardTs(ts uint64) {\n\to.Lock()\n\tdefer o.Unlock()\n\to.discardTs = ts\n}", "code_tokens": ["func", "(", "o", "*", "oracle", ")", "setDiscardTs", "(", "ts", "uint64", ")", "{", "o", ".", "Lock", "(", ")", "\n", "defer", "o", ".", "Unlock", "(", ")", "\n", "o", ".", "discardTs", "=", "ts", "\n", "}"], "docstring": "// Any deleted or invalid versions at or below ts would be discarded during\n// compaction to reclaim disk space in LSM tree and thence value log.", "docstring_tokens": ["Any", "deleted", "or", "invalid", "versions", "at", "or", "below", "ts", "would", "be", "discarded", "during", "compaction", "to", "reclaim", "disk", "space", "in", "LSM", "tree", "and", "thence", "value", "log", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/txn.go#L132-L136", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "txn.go", "func_name": "hasConflict", "original_string": "func (o *oracle) hasConflict(txn *Txn) bool {\n\tif len(txn.reads) == 0 {\n\t\treturn false\n\t}\n\tfor _, ro := range txn.reads {\n\t\t// A commit at the read timestamp is expected.\n\t\t// But, any commit after the read timestamp should cause a conflict.\n\t\tif ts, has := o.commits[ro]; has && ts > txn.readTs {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}", "language": "go", "code": "func (o *oracle) hasConflict(txn *Txn) bool {\n\tif len(txn.reads) == 0 {\n\t\treturn false\n\t}\n\tfor _, ro := range txn.reads {\n\t\t// A commit at the read timestamp is expected.\n\t\t// But, any commit after the read timestamp should cause a conflict.\n\t\tif ts, has := o.commits[ro]; has && ts > txn.readTs {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}", "code_tokens": ["func", "(", "o", "*", "oracle", ")", "hasConflict", "(", "txn", "*", "Txn", ")", "bool", "{", "if", "len", "(", "txn", ".", "reads", ")", "==", "0", "{", "return", "false", "\n", "}", "\n", "for", "_", ",", "ro", ":=", "range", "txn", ".", "reads", "{", "// A commit at the read timestamp is expected.", "// But, any commit after the read timestamp should cause a conflict.", "if", "ts", ",", "has", ":=", "o", ".", "commits", "[", "ro", "]", ";", "has", "&&", "ts", ">", "txn", ".", "readTs", "{", "return", "true", "\n", "}", "\n", "}", "\n", "return", "false", "\n", "}"], "docstring": "// hasConflict must be called while having a lock.", "docstring_tokens": ["hasConflict", "must", "be", "called", "while", "having", "a", "lock", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/txn.go#L148-L160", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "txn.go", "func_name": "Set", "original_string": "func (txn *Txn) Set(key, val []byte) error {\n\te := &Entry{\n\t\tKey:   key,\n\t\tValue: val,\n\t}\n\treturn txn.SetEntry(e)\n}", "language": "go", "code": "func (txn *Txn) Set(key, val []byte) error {\n\te := &Entry{\n\t\tKey:   key,\n\t\tValue: val,\n\t}\n\treturn txn.SetEntry(e)\n}", "code_tokens": ["func", "(", "txn", "*", "Txn", ")", "Set", "(", "key", ",", "val", "[", "]", "byte", ")", "error", "{", "e", ":=", "&", "Entry", "{", "Key", ":", "key", ",", "Value", ":", "val", ",", "}", "\n", "return", "txn", ".", "SetEntry", "(", "e", ")", "\n", "}"], "docstring": "// Set adds a key-value pair to the database.\n//\n// It will return ErrReadOnlyTxn if update flag was set to false when creating the\n// transaction.\n//\n// The current transaction keeps a reference to the key and val byte slice\n// arguments. Users must not modify key and val until the end of the transaction.", "docstring_tokens": ["Set", "adds", "a", "key", "-", "value", "pair", "to", "the", "database", ".", "It", "will", "return", "ErrReadOnlyTxn", "if", "update", "flag", "was", "set", "to", "false", "when", "creating", "the", "transaction", ".", "The", "current", "transaction", "keeps", "a", "reference", "to", "the", "key", "and", "val", "byte", "slice", "arguments", ".", "Users", "must", "not", "modify", "key", "and", "val", "until", "the", "end", "of", "the", "transaction", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/txn.go#L308-L314", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "table/iterator.go", "func_name": "Seek", "original_string": "func (itr *blockIterator) Seek(key []byte, whence int) {\n\titr.err = nil\n\n\tswitch whence {\n\tcase origin:\n\t\titr.Reset()\n\tcase current:\n\t}\n\n\tvar done bool\n\tfor itr.Init(); itr.Valid(); itr.Next() {\n\t\tk := itr.Key()\n\t\tif y.CompareKeys(k, key) >= 0 {\n\t\t\t// We are done as k is >= key.\n\t\t\tdone = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif !done {\n\t\titr.err = io.EOF\n\t}\n}", "language": "go", "code": "func (itr *blockIterator) Seek(key []byte, whence int) {\n\titr.err = nil\n\n\tswitch whence {\n\tcase origin:\n\t\titr.Reset()\n\tcase current:\n\t}\n\n\tvar done bool\n\tfor itr.Init(); itr.Valid(); itr.Next() {\n\t\tk := itr.Key()\n\t\tif y.CompareKeys(k, key) >= 0 {\n\t\t\t// We are done as k is >= key.\n\t\t\tdone = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif !done {\n\t\titr.err = io.EOF\n\t}\n}", "code_tokens": ["func", "(", "itr", "*", "blockIterator", ")", "Seek", "(", "key", "[", "]", "byte", ",", "whence", "int", ")", "{", "itr", ".", "err", "=", "nil", "\n\n", "switch", "whence", "{", "case", "origin", ":", "itr", ".", "Reset", "(", ")", "\n", "case", "current", ":", "}", "\n\n", "var", "done", "bool", "\n", "for", "itr", ".", "Init", "(", ")", ";", "itr", ".", "Valid", "(", ")", ";", "itr", ".", "Next", "(", ")", "{", "k", ":=", "itr", ".", "Key", "(", ")", "\n", "if", "y", ".", "CompareKeys", "(", "k", ",", "key", ")", ">=", "0", "{", "// We are done as k is >= key.", "done", "=", "true", "\n", "break", "\n", "}", "\n", "}", "\n", "if", "!", "done", "{", "itr", ".", "err", "=", "io", ".", "EOF", "\n", "}", "\n", "}"], "docstring": "// Seek brings us to the first block element that is >= input key.", "docstring_tokens": ["Seek", "brings", "us", "to", "the", "first", "block", "element", "that", "is", ">", "=", "input", "key", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/table/iterator.go#L74-L95", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "table/iterator.go", "func_name": "SeekToLast", "original_string": "func (itr *blockIterator) SeekToLast() {\n\titr.err = nil\n\tfor itr.Init(); itr.Valid(); itr.Next() {\n\t}\n\titr.Prev()\n}", "language": "go", "code": "func (itr *blockIterator) SeekToLast() {\n\titr.err = nil\n\tfor itr.Init(); itr.Valid(); itr.Next() {\n\t}\n\titr.Prev()\n}", "code_tokens": ["func", "(", "itr", "*", "blockIterator", ")", "SeekToLast", "(", ")", "{", "itr", ".", "err", "=", "nil", "\n", "for", "itr", ".", "Init", "(", ")", ";", "itr", ".", "Valid", "(", ")", ";", "itr", ".", "Next", "(", ")", "{", "}", "\n", "itr", ".", "Prev", "(", ")", "\n", "}"], "docstring": "// SeekToLast brings us to the last element. Valid should return true.", "docstring_tokens": ["SeekToLast", "brings", "us", "to", "the", "last", "element", ".", "Valid", "should", "return", "true", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/table/iterator.go#L103-L108", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "table/iterator.go", "func_name": "parseKV", "original_string": "func (itr *blockIterator) parseKV(h header) {\n\tif cap(itr.key) < int(h.plen+h.klen) {\n\t\tsz := int(h.plen) + int(h.klen) // Convert to int before adding to avoid uint16 overflow.\n\t\titr.key = make([]byte, 2*sz)\n\t}\n\titr.key = itr.key[:h.plen+h.klen]\n\tcopy(itr.key, itr.baseKey[:h.plen])\n\tcopy(itr.key[h.plen:], itr.data[itr.pos:itr.pos+uint32(h.klen)])\n\titr.pos += uint32(h.klen)\n\n\tif itr.pos+uint32(h.vlen) > uint32(len(itr.data)) {\n\t\titr.err = errors.Errorf(\"Value exceeded size of block: %d %d %d %d %v\",\n\t\t\titr.pos, h.klen, h.vlen, len(itr.data), h)\n\t\treturn\n\t}\n\titr.val = y.SafeCopy(itr.val, itr.data[itr.pos:itr.pos+uint32(h.vlen)])\n\titr.pos += uint32(h.vlen)\n}", "language": "go", "code": "func (itr *blockIterator) parseKV(h header) {\n\tif cap(itr.key) < int(h.plen+h.klen) {\n\t\tsz := int(h.plen) + int(h.klen) // Convert to int before adding to avoid uint16 overflow.\n\t\titr.key = make([]byte, 2*sz)\n\t}\n\titr.key = itr.key[:h.plen+h.klen]\n\tcopy(itr.key, itr.baseKey[:h.plen])\n\tcopy(itr.key[h.plen:], itr.data[itr.pos:itr.pos+uint32(h.klen)])\n\titr.pos += uint32(h.klen)\n\n\tif itr.pos+uint32(h.vlen) > uint32(len(itr.data)) {\n\t\titr.err = errors.Errorf(\"Value exceeded size of block: %d %d %d %d %v\",\n\t\t\titr.pos, h.klen, h.vlen, len(itr.data), h)\n\t\treturn\n\t}\n\titr.val = y.SafeCopy(itr.val, itr.data[itr.pos:itr.pos+uint32(h.vlen)])\n\titr.pos += uint32(h.vlen)\n}", "code_tokens": ["func", "(", "itr", "*", "blockIterator", ")", "parseKV", "(", "h", "header", ")", "{", "if", "cap", "(", "itr", ".", "key", ")", "<", "int", "(", "h", ".", "plen", "+", "h", ".", "klen", ")", "{", "sz", ":=", "int", "(", "h", ".", "plen", ")", "+", "int", "(", "h", ".", "klen", ")", "// Convert to int before adding to avoid uint16 overflow.", "\n", "itr", ".", "key", "=", "make", "(", "[", "]", "byte", ",", "2", "*", "sz", ")", "\n", "}", "\n", "itr", ".", "key", "=", "itr", ".", "key", "[", ":", "h", ".", "plen", "+", "h", ".", "klen", "]", "\n", "copy", "(", "itr", ".", "key", ",", "itr", ".", "baseKey", "[", ":", "h", ".", "plen", "]", ")", "\n", "copy", "(", "itr", ".", "key", "[", "h", ".", "plen", ":", "]", ",", "itr", ".", "data", "[", "itr", ".", "pos", ":", "itr", ".", "pos", "+", "uint32", "(", "h", ".", "klen", ")", "]", ")", "\n", "itr", ".", "pos", "+=", "uint32", "(", "h", ".", "klen", ")", "\n\n", "if", "itr", ".", "pos", "+", "uint32", "(", "h", ".", "vlen", ")", ">", "uint32", "(", "len", "(", "itr", ".", "data", ")", ")", "{", "itr", ".", "err", "=", "errors", ".", "Errorf", "(", "\"", "\"", ",", "itr", ".", "pos", ",", "h", ".", "klen", ",", "h", ".", "vlen", ",", "len", "(", "itr", ".", "data", ")", ",", "h", ")", "\n", "return", "\n", "}", "\n", "itr", ".", "val", "=", "y", ".", "SafeCopy", "(", "itr", ".", "val", ",", "itr", ".", "data", "[", "itr", ".", "pos", ":", "itr", ".", "pos", "+", "uint32", "(", "h", ".", "vlen", ")", "]", ")", "\n", "itr", ".", "pos", "+=", "uint32", "(", "h", ".", "vlen", ")", "\n", "}"], "docstring": "// parseKV would allocate a new byte slice for key and for value.", "docstring_tokens": ["parseKV", "would", "allocate", "a", "new", "byte", "slice", "for", "key", "and", "for", "value", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/table/iterator.go#L111-L128", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "y/y.go", "func_name": "OpenExistingFile", "original_string": "func OpenExistingFile(filename string, flags uint32) (*os.File, error) {\n\topenFlags := os.O_RDWR\n\tif flags&ReadOnly != 0 {\n\t\topenFlags = os.O_RDONLY\n\t}\n\n\tif flags&Sync != 0 {\n\t\topenFlags |= datasyncFileFlag\n\t}\n\treturn os.OpenFile(filename, openFlags, 0)\n}", "language": "go", "code": "func OpenExistingFile(filename string, flags uint32) (*os.File, error) {\n\topenFlags := os.O_RDWR\n\tif flags&ReadOnly != 0 {\n\t\topenFlags = os.O_RDONLY\n\t}\n\n\tif flags&Sync != 0 {\n\t\topenFlags |= datasyncFileFlag\n\t}\n\treturn os.OpenFile(filename, openFlags, 0)\n}", "code_tokens": ["func", "OpenExistingFile", "(", "filename", "string", ",", "flags", "uint32", ")", "(", "*", "os", ".", "File", ",", "error", ")", "{", "openFlags", ":=", "os", ".", "O_RDWR", "\n", "if", "flags", "&", "ReadOnly", "!=", "0", "{", "openFlags", "=", "os", ".", "O_RDONLY", "\n", "}", "\n\n", "if", "flags", "&", "Sync", "!=", "0", "{", "openFlags", "|=", "datasyncFileFlag", "\n", "}", "\n", "return", "os", ".", "OpenFile", "(", "filename", ",", "openFlags", ",", "0", ")", "\n", "}"], "docstring": "// OpenExistingFile opens an existing file, errors if it doesn't exist.", "docstring_tokens": ["OpenExistingFile", "opens", "an", "existing", "file", "errors", "if", "it", "doesn", "t", "exist", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/y/y.go#L57-L67", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "y/y.go", "func_name": "CreateSyncedFile", "original_string": "func CreateSyncedFile(filename string, sync bool) (*os.File, error) {\n\tflags := os.O_RDWR | os.O_CREATE | os.O_EXCL\n\tif sync {\n\t\tflags |= datasyncFileFlag\n\t}\n\treturn os.OpenFile(filename, flags, 0666)\n}", "language": "go", "code": "func CreateSyncedFile(filename string, sync bool) (*os.File, error) {\n\tflags := os.O_RDWR | os.O_CREATE | os.O_EXCL\n\tif sync {\n\t\tflags |= datasyncFileFlag\n\t}\n\treturn os.OpenFile(filename, flags, 0666)\n}", "code_tokens": ["func", "CreateSyncedFile", "(", "filename", "string", ",", "sync", "bool", ")", "(", "*", "os", ".", "File", ",", "error", ")", "{", "flags", ":=", "os", ".", "O_RDWR", "|", "os", ".", "O_CREATE", "|", "os", ".", "O_EXCL", "\n", "if", "sync", "{", "flags", "|=", "datasyncFileFlag", "\n", "}", "\n", "return", "os", ".", "OpenFile", "(", "filename", ",", "flags", ",", "0666", ")", "\n", "}"], "docstring": "// CreateSyncedFile creates a new file (using O_EXCL), errors if it already existed.", "docstring_tokens": ["CreateSyncedFile", "creates", "a", "new", "file", "(", "using", "O_EXCL", ")", "errors", "if", "it", "already", "existed", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/y/y.go#L70-L76", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "y/y.go", "func_name": "Copy", "original_string": "func Copy(a []byte) []byte {\n\tb := make([]byte, len(a))\n\tcopy(b, a)\n\treturn b\n}", "language": "go", "code": "func Copy(a []byte) []byte {\n\tb := make([]byte, len(a))\n\tcopy(b, a)\n\treturn b\n}", "code_tokens": ["func", "Copy", "(", "a", "[", "]", "byte", ")", "[", "]", "byte", "{", "b", ":=", "make", "(", "[", "]", "byte", ",", "len", "(", "a", ")", ")", "\n", "copy", "(", "b", ",", "a", ")", "\n", "return", "b", "\n", "}"], "docstring": "// Copy copies a byte slice and returns the copied slice.", "docstring_tokens": ["Copy", "copies", "a", "byte", "slice", "and", "returns", "the", "copied", "slice", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/y/y.go#L102-L106", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "managed_db.go", "func_name": "OpenManaged", "original_string": "func OpenManaged(opts Options) (*DB, error) {\n\topts.managedTxns = true\n\treturn Open(opts)\n}", "language": "go", "code": "func OpenManaged(opts Options) (*DB, error) {\n\topts.managedTxns = true\n\treturn Open(opts)\n}", "code_tokens": ["func", "OpenManaged", "(", "opts", "Options", ")", "(", "*", "DB", ",", "error", ")", "{", "opts", ".", "managedTxns", "=", "true", "\n", "return", "Open", "(", "opts", ")", "\n", "}"], "docstring": "// OpenManaged returns a new DB, which allows more control over setting\n// transaction timestamps, aka managed mode.\n//\n// This is only useful for databases built on top of Badger (like Dgraph), and\n// can be ignored by most users.", "docstring_tokens": ["OpenManaged", "returns", "a", "new", "DB", "which", "allows", "more", "control", "over", "setting", "transaction", "timestamps", "aka", "managed", "mode", ".", "This", "is", "only", "useful", "for", "databases", "built", "on", "top", "of", "Badger", "(", "like", "Dgraph", ")", "and", "can", "be", "ignored", "by", "most", "users", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/managed_db.go#L24-L27", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "managed_db.go", "func_name": "NewTransactionAt", "original_string": "func (db *DB) NewTransactionAt(readTs uint64, update bool) *Txn {\n\tif !db.opt.managedTxns {\n\t\tpanic(\"Cannot use NewTransactionAt with managedDB=false. Use NewTransaction instead.\")\n\t}\n\ttxn := db.newTransaction(update, true)\n\ttxn.readTs = readTs\n\treturn txn\n}", "language": "go", "code": "func (db *DB) NewTransactionAt(readTs uint64, update bool) *Txn {\n\tif !db.opt.managedTxns {\n\t\tpanic(\"Cannot use NewTransactionAt with managedDB=false. Use NewTransaction instead.\")\n\t}\n\ttxn := db.newTransaction(update, true)\n\ttxn.readTs = readTs\n\treturn txn\n}", "code_tokens": ["func", "(", "db", "*", "DB", ")", "NewTransactionAt", "(", "readTs", "uint64", ",", "update", "bool", ")", "*", "Txn", "{", "if", "!", "db", ".", "opt", ".", "managedTxns", "{", "panic", "(", "\"", "\"", ")", "\n", "}", "\n", "txn", ":=", "db", ".", "newTransaction", "(", "update", ",", "true", ")", "\n", "txn", ".", "readTs", "=", "readTs", "\n", "return", "txn", "\n", "}"], "docstring": "// NewTransactionAt follows the same logic as DB.NewTransaction(), but uses the\n// provided read timestamp.\n//\n// This is only useful for databases built on top of Badger (like Dgraph), and\n// can be ignored by most users.", "docstring_tokens": ["NewTransactionAt", "follows", "the", "same", "logic", "as", "DB", ".", "NewTransaction", "()", "but", "uses", "the", "provided", "read", "timestamp", ".", "This", "is", "only", "useful", "for", "databases", "built", "on", "top", "of", "Badger", "(", "like", "Dgraph", ")", "and", "can", "be", "ignored", "by", "most", "users", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/managed_db.go#L34-L41", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "managed_db.go", "func_name": "CommitAt", "original_string": "func (txn *Txn) CommitAt(commitTs uint64, callback func(error)) error {\n\tif !txn.db.opt.managedTxns {\n\t\tpanic(\"Cannot use CommitAt with managedDB=false. Use Commit instead.\")\n\t}\n\ttxn.commitTs = commitTs\n\tif callback == nil {\n\t\treturn txn.Commit()\n\t}\n\ttxn.CommitWith(callback)\n\treturn nil\n}", "language": "go", "code": "func (txn *Txn) CommitAt(commitTs uint64, callback func(error)) error {\n\tif !txn.db.opt.managedTxns {\n\t\tpanic(\"Cannot use CommitAt with managedDB=false. Use Commit instead.\")\n\t}\n\ttxn.commitTs = commitTs\n\tif callback == nil {\n\t\treturn txn.Commit()\n\t}\n\ttxn.CommitWith(callback)\n\treturn nil\n}", "code_tokens": ["func", "(", "txn", "*", "Txn", ")", "CommitAt", "(", "commitTs", "uint64", ",", "callback", "func", "(", "error", ")", ")", "error", "{", "if", "!", "txn", ".", "db", ".", "opt", ".", "managedTxns", "{", "panic", "(", "\"", "\"", ")", "\n", "}", "\n", "txn", ".", "commitTs", "=", "commitTs", "\n", "if", "callback", "==", "nil", "{", "return", "txn", ".", "Commit", "(", ")", "\n", "}", "\n", "txn", ".", "CommitWith", "(", "callback", ")", "\n", "return", "nil", "\n", "}"], "docstring": "// CommitAt commits the transaction, following the same logic as Commit(), but\n// at the given commit timestamp. This will panic if not used with managed transactions.\n//\n// This is only useful for databases built on top of Badger (like Dgraph), and\n// can be ignored by most users.", "docstring_tokens": ["CommitAt", "commits", "the", "transaction", "following", "the", "same", "logic", "as", "Commit", "()", "but", "at", "the", "given", "commit", "timestamp", ".", "This", "will", "panic", "if", "not", "used", "with", "managed", "transactions", ".", "This", "is", "only", "useful", "for", "databases", "built", "on", "top", "of", "Badger", "(", "like", "Dgraph", ")", "and", "can", "be", "ignored", "by", "most", "users", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/managed_db.go#L48-L58", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "value.go", "func_name": "openReadOnly", "original_string": "func (lf *logFile) openReadOnly() error {\n\tvar err error\n\tlf.fd, err = os.OpenFile(lf.path, os.O_RDONLY, 0666)\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"Unable to open %q as RDONLY.\", lf.path)\n\t}\n\n\tfi, err := lf.fd.Stat()\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"Unable to check stat for %q\", lf.path)\n\t}\n\ty.AssertTrue(fi.Size() <= math.MaxUint32)\n\tlf.size = uint32(fi.Size())\n\n\tif err = lf.mmap(fi.Size()); err != nil {\n\t\t_ = lf.fd.Close()\n\t\treturn y.Wrapf(err, \"Unable to map file\")\n\t}\n\n\treturn nil\n}", "language": "go", "code": "func (lf *logFile) openReadOnly() error {\n\tvar err error\n\tlf.fd, err = os.OpenFile(lf.path, os.O_RDONLY, 0666)\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"Unable to open %q as RDONLY.\", lf.path)\n\t}\n\n\tfi, err := lf.fd.Stat()\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"Unable to check stat for %q\", lf.path)\n\t}\n\ty.AssertTrue(fi.Size() <= math.MaxUint32)\n\tlf.size = uint32(fi.Size())\n\n\tif err = lf.mmap(fi.Size()); err != nil {\n\t\t_ = lf.fd.Close()\n\t\treturn y.Wrapf(err, \"Unable to map file\")\n\t}\n\n\treturn nil\n}", "code_tokens": ["func", "(", "lf", "*", "logFile", ")", "openReadOnly", "(", ")", "error", "{", "var", "err", "error", "\n", "lf", ".", "fd", ",", "err", "=", "os", ".", "OpenFile", "(", "lf", ".", "path", ",", "os", ".", "O_RDONLY", ",", "0666", ")", "\n", "if", "err", "!=", "nil", "{", "return", "errors", ".", "Wrapf", "(", "err", ",", "\"", "\"", ",", "lf", ".", "path", ")", "\n", "}", "\n\n", "fi", ",", "err", ":=", "lf", ".", "fd", ".", "Stat", "(", ")", "\n", "if", "err", "!=", "nil", "{", "return", "errors", ".", "Wrapf", "(", "err", ",", "\"", "\"", ",", "lf", ".", "path", ")", "\n", "}", "\n", "y", ".", "AssertTrue", "(", "fi", ".", "Size", "(", ")", "<=", "math", ".", "MaxUint32", ")", "\n", "lf", ".", "size", "=", "uint32", "(", "fi", ".", "Size", "(", ")", ")", "\n\n", "if", "err", "=", "lf", ".", "mmap", "(", "fi", ".", "Size", "(", ")", ")", ";", "err", "!=", "nil", "{", "_", "=", "lf", ".", "fd", ".", "Close", "(", ")", "\n", "return", "y", ".", "Wrapf", "(", "err", ",", "\"", "\"", ")", "\n", "}", "\n\n", "return", "nil", "\n", "}"], "docstring": "// openReadOnly assumes that we have a write lock on logFile.", "docstring_tokens": ["openReadOnly", "assumes", "that", "we", "have", "a", "write", "lock", "on", "logFile", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/value.go#L74-L94", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "value.go", "func_name": "read", "original_string": "func (lf *logFile) read(p valuePointer, s *y.Slice) (buf []byte, err error) {\n\tvar nbr int64\n\toffset := p.Offset\n\tif lf.loadingMode == options.FileIO {\n\t\tbuf = s.Resize(int(p.Len))\n\t\tvar n int\n\t\tn, err = lf.fd.ReadAt(buf, int64(offset))\n\t\tnbr = int64(n)\n\t} else {\n\t\t// Do not convert size to uint32, because the lf.fmap can be of size\n\t\t// 4GB, which overflows the uint32 during conversion to make the size 0,\n\t\t// causing the read to fail with ErrEOF. See issue #585.\n\t\tsize := int64(len(lf.fmap))\n\t\tvalsz := p.Len\n\t\tif int64(offset) >= size || int64(offset+valsz) > size {\n\t\t\terr = y.ErrEOF\n\t\t} else {\n\t\t\tbuf = lf.fmap[offset : offset+valsz]\n\t\t\tnbr = int64(valsz)\n\t\t}\n\t}\n\ty.NumReads.Add(1)\n\ty.NumBytesRead.Add(nbr)\n\treturn buf, err\n}", "language": "go", "code": "func (lf *logFile) read(p valuePointer, s *y.Slice) (buf []byte, err error) {\n\tvar nbr int64\n\toffset := p.Offset\n\tif lf.loadingMode == options.FileIO {\n\t\tbuf = s.Resize(int(p.Len))\n\t\tvar n int\n\t\tn, err = lf.fd.ReadAt(buf, int64(offset))\n\t\tnbr = int64(n)\n\t} else {\n\t\t// Do not convert size to uint32, because the lf.fmap can be of size\n\t\t// 4GB, which overflows the uint32 during conversion to make the size 0,\n\t\t// causing the read to fail with ErrEOF. See issue #585.\n\t\tsize := int64(len(lf.fmap))\n\t\tvalsz := p.Len\n\t\tif int64(offset) >= size || int64(offset+valsz) > size {\n\t\t\terr = y.ErrEOF\n\t\t} else {\n\t\t\tbuf = lf.fmap[offset : offset+valsz]\n\t\t\tnbr = int64(valsz)\n\t\t}\n\t}\n\ty.NumReads.Add(1)\n\ty.NumBytesRead.Add(nbr)\n\treturn buf, err\n}", "code_tokens": ["func", "(", "lf", "*", "logFile", ")", "read", "(", "p", "valuePointer", ",", "s", "*", "y", ".", "Slice", ")", "(", "buf", "[", "]", "byte", ",", "err", "error", ")", "{", "var", "nbr", "int64", "\n", "offset", ":=", "p", ".", "Offset", "\n", "if", "lf", ".", "loadingMode", "==", "options", ".", "FileIO", "{", "buf", "=", "s", ".", "Resize", "(", "int", "(", "p", ".", "Len", ")", ")", "\n", "var", "n", "int", "\n", "n", ",", "err", "=", "lf", ".", "fd", ".", "ReadAt", "(", "buf", ",", "int64", "(", "offset", ")", ")", "\n", "nbr", "=", "int64", "(", "n", ")", "\n", "}", "else", "{", "// Do not convert size to uint32, because the lf.fmap can be of size", "// 4GB, which overflows the uint32 during conversion to make the size 0,", "// causing the read to fail with ErrEOF. See issue #585.", "size", ":=", "int64", "(", "len", "(", "lf", ".", "fmap", ")", ")", "\n", "valsz", ":=", "p", ".", "Len", "\n", "if", "int64", "(", "offset", ")", ">=", "size", "||", "int64", "(", "offset", "+", "valsz", ")", ">", "size", "{", "err", "=", "y", ".", "ErrEOF", "\n", "}", "else", "{", "buf", "=", "lf", ".", "fmap", "[", "offset", ":", "offset", "+", "valsz", "]", "\n", "nbr", "=", "int64", "(", "valsz", ")", "\n", "}", "\n", "}", "\n", "y", ".", "NumReads", ".", "Add", "(", "1", ")", "\n", "y", ".", "NumBytesRead", ".", "Add", "(", "nbr", ")", "\n", "return", "buf", ",", "err", "\n", "}"], "docstring": "// Acquire lock on mmap/file if you are calling this", "docstring_tokens": ["Acquire", "lock", "on", "mmap", "/", "file", "if", "you", "are", "calling", "this"], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/value.go#L120-L144", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "backup.go", "func_name": "Backup", "original_string": "func (db *DB) Backup(w io.Writer, since uint64) (uint64, error) {\n\tstream := db.NewStream()\n\tstream.LogPrefix = \"DB.Backup\"\n\treturn stream.Backup(w, since)\n}", "language": "go", "code": "func (db *DB) Backup(w io.Writer, since uint64) (uint64, error) {\n\tstream := db.NewStream()\n\tstream.LogPrefix = \"DB.Backup\"\n\treturn stream.Backup(w, since)\n}", "code_tokens": ["func", "(", "db", "*", "DB", ")", "Backup", "(", "w", "io", ".", "Writer", ",", "since", "uint64", ")", "(", "uint64", ",", "error", ")", "{", "stream", ":=", "db", ".", "NewStream", "(", ")", "\n", "stream", ".", "LogPrefix", "=", "\"", "\"", "\n", "return", "stream", ".", "Backup", "(", "w", ",", "since", ")", "\n", "}"], "docstring": "// Backup is a wrapper function over Stream.Backup to generate full and incremental backups of the\n// DB. For more control over how many goroutines are used to generate the backup, or if you wish to\n// backup only a certain range of keys, use Stream.Backup directly.", "docstring_tokens": ["Backup", "is", "a", "wrapper", "function", "over", "Stream", ".", "Backup", "to", "generate", "full", "and", "incremental", "backups", "of", "the", "DB", ".", "For", "more", "control", "over", "how", "many", "goroutines", "are", "used", "to", "generate", "the", "backup", "or", "if", "you", "wish", "to", "backup", "only", "a", "certain", "range", "of", "keys", "use", "Stream", ".", "Backup", "directly", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/backup.go#L34-L38", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "backup.go", "func_name": "Backup", "original_string": "func (stream *Stream) Backup(w io.Writer, since uint64) (uint64, error) {\n\tstream.KeyToList = func(key []byte, itr *Iterator) (*pb.KVList, error) {\n\t\tlist := &pb.KVList{}\n\t\tfor ; itr.Valid(); itr.Next() {\n\t\t\titem := itr.Item()\n\t\t\tif !bytes.Equal(item.Key(), key) {\n\t\t\t\treturn list, nil\n\t\t\t}\n\t\t\tif item.Version() < since {\n\t\t\t\t// Ignore versions less than given timestamp, or skip older\n\t\t\t\t// versions of the given key.\n\t\t\t\treturn list, nil\n\t\t\t}\n\n\t\t\tvar valCopy []byte\n\t\t\tif !item.IsDeletedOrExpired() {\n\t\t\t\t// No need to copy value, if item is deleted or expired.\n\t\t\t\tvar err error\n\t\t\t\tvalCopy, err = item.ValueCopy(nil)\n\t\t\t\tif err != nil {\n\t\t\t\t\tstream.db.opt.Errorf(\"Key [%x, %d]. Error while fetching value [%v]\\n\",\n\t\t\t\t\t\titem.Key(), item.Version(), err)\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// clear txn bits\n\t\t\tmeta := item.meta &^ (bitTxn | bitFinTxn)\n\t\t\tkv := &pb.KV{\n\t\t\t\tKey:       item.KeyCopy(nil),\n\t\t\t\tValue:     valCopy,\n\t\t\t\tUserMeta:  []byte{item.UserMeta()},\n\t\t\t\tVersion:   item.Version(),\n\t\t\t\tExpiresAt: item.ExpiresAt(),\n\t\t\t\tMeta:      []byte{meta},\n\t\t\t}\n\t\t\tlist.Kv = append(list.Kv, kv)\n\n\t\t\tswitch {\n\t\t\tcase item.DiscardEarlierVersions():\n\t\t\t\t// If we need to discard earlier versions of this item, add a delete\n\t\t\t\t// marker just below the current version.\n\t\t\t\tlist.Kv = append(list.Kv, &pb.KV{\n\t\t\t\t\tKey:     item.KeyCopy(nil),\n\t\t\t\t\tVersion: item.Version() - 1,\n\t\t\t\t\tMeta:    []byte{bitDelete},\n\t\t\t\t})\n\t\t\t\treturn list, nil\n\n\t\t\tcase item.IsDeletedOrExpired():\n\t\t\t\treturn list, nil\n\t\t\t}\n\t\t}\n\t\treturn list, nil\n\t}\n\n\tvar maxVersion uint64\n\tstream.Send = func(list *pb.KVList) error {\n\t\tfor _, kv := range list.Kv {\n\t\t\tif maxVersion < kv.Version {\n\t\t\t\tmaxVersion = kv.Version\n\t\t\t}\n\t\t\tif err := writeTo(kv, w); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n\n\tif err := stream.Orchestrate(context.Background()); err != nil {\n\t\treturn 0, err\n\t}\n\treturn maxVersion, nil\n}", "language": "go", "code": "func (stream *Stream) Backup(w io.Writer, since uint64) (uint64, error) {\n\tstream.KeyToList = func(key []byte, itr *Iterator) (*pb.KVList, error) {\n\t\tlist := &pb.KVList{}\n\t\tfor ; itr.Valid(); itr.Next() {\n\t\t\titem := itr.Item()\n\t\t\tif !bytes.Equal(item.Key(), key) {\n\t\t\t\treturn list, nil\n\t\t\t}\n\t\t\tif item.Version() < since {\n\t\t\t\t// Ignore versions less than given timestamp, or skip older\n\t\t\t\t// versions of the given key.\n\t\t\t\treturn list, nil\n\t\t\t}\n\n\t\t\tvar valCopy []byte\n\t\t\tif !item.IsDeletedOrExpired() {\n\t\t\t\t// No need to copy value, if item is deleted or expired.\n\t\t\t\tvar err error\n\t\t\t\tvalCopy, err = item.ValueCopy(nil)\n\t\t\t\tif err != nil {\n\t\t\t\t\tstream.db.opt.Errorf(\"Key [%x, %d]. Error while fetching value [%v]\\n\",\n\t\t\t\t\t\titem.Key(), item.Version(), err)\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// clear txn bits\n\t\t\tmeta := item.meta &^ (bitTxn | bitFinTxn)\n\t\t\tkv := &pb.KV{\n\t\t\t\tKey:       item.KeyCopy(nil),\n\t\t\t\tValue:     valCopy,\n\t\t\t\tUserMeta:  []byte{item.UserMeta()},\n\t\t\t\tVersion:   item.Version(),\n\t\t\t\tExpiresAt: item.ExpiresAt(),\n\t\t\t\tMeta:      []byte{meta},\n\t\t\t}\n\t\t\tlist.Kv = append(list.Kv, kv)\n\n\t\t\tswitch {\n\t\t\tcase item.DiscardEarlierVersions():\n\t\t\t\t// If we need to discard earlier versions of this item, add a delete\n\t\t\t\t// marker just below the current version.\n\t\t\t\tlist.Kv = append(list.Kv, &pb.KV{\n\t\t\t\t\tKey:     item.KeyCopy(nil),\n\t\t\t\t\tVersion: item.Version() - 1,\n\t\t\t\t\tMeta:    []byte{bitDelete},\n\t\t\t\t})\n\t\t\t\treturn list, nil\n\n\t\t\tcase item.IsDeletedOrExpired():\n\t\t\t\treturn list, nil\n\t\t\t}\n\t\t}\n\t\treturn list, nil\n\t}\n\n\tvar maxVersion uint64\n\tstream.Send = func(list *pb.KVList) error {\n\t\tfor _, kv := range list.Kv {\n\t\t\tif maxVersion < kv.Version {\n\t\t\t\tmaxVersion = kv.Version\n\t\t\t}\n\t\t\tif err := writeTo(kv, w); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n\n\tif err := stream.Orchestrate(context.Background()); err != nil {\n\t\treturn 0, err\n\t}\n\treturn maxVersion, nil\n}", "code_tokens": ["func", "(", "stream", "*", "Stream", ")", "Backup", "(", "w", "io", ".", "Writer", ",", "since", "uint64", ")", "(", "uint64", ",", "error", ")", "{", "stream", ".", "KeyToList", "=", "func", "(", "key", "[", "]", "byte", ",", "itr", "*", "Iterator", ")", "(", "*", "pb", ".", "KVList", ",", "error", ")", "{", "list", ":=", "&", "pb", ".", "KVList", "{", "}", "\n", "for", ";", "itr", ".", "Valid", "(", ")", ";", "itr", ".", "Next", "(", ")", "{", "item", ":=", "itr", ".", "Item", "(", ")", "\n", "if", "!", "bytes", ".", "Equal", "(", "item", ".", "Key", "(", ")", ",", "key", ")", "{", "return", "list", ",", "nil", "\n", "}", "\n", "if", "item", ".", "Version", "(", ")", "<", "since", "{", "// Ignore versions less than given timestamp, or skip older", "// versions of the given key.", "return", "list", ",", "nil", "\n", "}", "\n\n", "var", "valCopy", "[", "]", "byte", "\n", "if", "!", "item", ".", "IsDeletedOrExpired", "(", ")", "{", "// No need to copy value, if item is deleted or expired.", "var", "err", "error", "\n", "valCopy", ",", "err", "=", "item", ".", "ValueCopy", "(", "nil", ")", "\n", "if", "err", "!=", "nil", "{", "stream", ".", "db", ".", "opt", ".", "Errorf", "(", "\"", "\\n", "\"", ",", "item", ".", "Key", "(", ")", ",", "item", ".", "Version", "(", ")", ",", "err", ")", "\n", "return", "nil", ",", "err", "\n", "}", "\n", "}", "\n\n", "// clear txn bits", "meta", ":=", "item", ".", "meta", "&^", "(", "bitTxn", "|", "bitFinTxn", ")", "\n", "kv", ":=", "&", "pb", ".", "KV", "{", "Key", ":", "item", ".", "KeyCopy", "(", "nil", ")", ",", "Value", ":", "valCopy", ",", "UserMeta", ":", "[", "]", "byte", "{", "item", ".", "UserMeta", "(", ")", "}", ",", "Version", ":", "item", ".", "Version", "(", ")", ",", "ExpiresAt", ":", "item", ".", "ExpiresAt", "(", ")", ",", "Meta", ":", "[", "]", "byte", "{", "meta", "}", ",", "}", "\n", "list", ".", "Kv", "=", "append", "(", "list", ".", "Kv", ",", "kv", ")", "\n\n", "switch", "{", "case", "item", ".", "DiscardEarlierVersions", "(", ")", ":", "// If we need to discard earlier versions of this item, add a delete", "// marker just below the current version.", "list", ".", "Kv", "=", "append", "(", "list", ".", "Kv", ",", "&", "pb", ".", "KV", "{", "Key", ":", "item", ".", "KeyCopy", "(", "nil", ")", ",", "Version", ":", "item", ".", "Version", "(", ")", "-", "1", ",", "Meta", ":", "[", "]", "byte", "{", "bitDelete", "}", ",", "}", ")", "\n", "return", "list", ",", "nil", "\n\n", "case", "item", ".", "IsDeletedOrExpired", "(", ")", ":", "return", "list", ",", "nil", "\n", "}", "\n", "}", "\n", "return", "list", ",", "nil", "\n", "}", "\n\n", "var", "maxVersion", "uint64", "\n", "stream", ".", "Send", "=", "func", "(", "list", "*", "pb", ".", "KVList", ")", "error", "{", "for", "_", ",", "kv", ":=", "range", "list", ".", "Kv", "{", "if", "maxVersion", "<", "kv", ".", "Version", "{", "maxVersion", "=", "kv", ".", "Version", "\n", "}", "\n", "if", "err", ":=", "writeTo", "(", "kv", ",", "w", ")", ";", "err", "!=", "nil", "{", "return", "err", "\n", "}", "\n", "}", "\n", "return", "nil", "\n", "}", "\n\n", "if", "err", ":=", "stream", ".", "Orchestrate", "(", "context", ".", "Background", "(", ")", ")", ";", "err", "!=", "nil", "{", "return", "0", ",", "err", "\n", "}", "\n", "return", "maxVersion", ",", "nil", "\n", "}"], "docstring": "// Backup dumps a protobuf-encoded list of all entries in the database into the\n// given writer, that are newer than the specified version. It returns a\n// timestamp indicating when the entries were dumped which can be passed into a\n// later invocation to generate an incremental dump, of entries that have been\n// added/modified since the last invocation of Stream.Backup().\n//\n// This can be used to backup the data in a database at a given point in time.", "docstring_tokens": ["Backup", "dumps", "a", "protobuf", "-", "encoded", "list", "of", "all", "entries", "in", "the", "database", "into", "the", "given", "writer", "that", "are", "newer", "than", "the", "specified", "version", ".", "It", "returns", "a", "timestamp", "indicating", "when", "the", "entries", "were", "dumped", "which", "can", "be", "passed", "into", "a", "later", "invocation", "to", "generate", "an", "incremental", "dump", "of", "entries", "that", "have", "been", "added", "/", "modified", "since", "the", "last", "invocation", "of", "Stream", ".", "Backup", "()", ".", "This", "can", "be", "used", "to", "backup", "the", "data", "in", "a", "database", "at", "a", "given", "point", "in", "time", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/backup.go#L47-L120", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "table/table.go", "func_name": "ParseFileID", "original_string": "func ParseFileID(name string) (uint64, bool) {\n\tname = path.Base(name)\n\tif !strings.HasSuffix(name, fileSuffix) {\n\t\treturn 0, false\n\t}\n\t//\tsuffix := name[len(fileSuffix):]\n\tname = strings.TrimSuffix(name, fileSuffix)\n\tid, err := strconv.Atoi(name)\n\tif err != nil {\n\t\treturn 0, false\n\t}\n\ty.AssertTrue(id >= 0)\n\treturn uint64(id), true\n}", "language": "go", "code": "func ParseFileID(name string) (uint64, bool) {\n\tname = path.Base(name)\n\tif !strings.HasSuffix(name, fileSuffix) {\n\t\treturn 0, false\n\t}\n\t//\tsuffix := name[len(fileSuffix):]\n\tname = strings.TrimSuffix(name, fileSuffix)\n\tid, err := strconv.Atoi(name)\n\tif err != nil {\n\t\treturn 0, false\n\t}\n\ty.AssertTrue(id >= 0)\n\treturn uint64(id), true\n}", "code_tokens": ["func", "ParseFileID", "(", "name", "string", ")", "(", "uint64", ",", "bool", ")", "{", "name", "=", "path", ".", "Base", "(", "name", ")", "\n", "if", "!", "strings", ".", "HasSuffix", "(", "name", ",", "fileSuffix", ")", "{", "return", "0", ",", "false", "\n", "}", "\n", "//\tsuffix := name[len(fileSuffix):]", "name", "=", "strings", ".", "TrimSuffix", "(", "name", ",", "fileSuffix", ")", "\n", "id", ",", "err", ":=", "strconv", ".", "Atoi", "(", "name", ")", "\n", "if", "err", "!=", "nil", "{", "return", "0", ",", "false", "\n", "}", "\n", "y", ".", "AssertTrue", "(", "id", ">=", "0", ")", "\n", "return", "uint64", "(", "id", ")", ",", "true", "\n", "}"], "docstring": "// ParseFileID reads the file id out of a filename.", "docstring_tokens": ["ParseFileID", "reads", "the", "file", "id", "out", "of", "a", "filename", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/table/table.go#L315-L328", "partition": "test"}
{"repo": "dgraph-io/badger", "path": "table/table.go", "func_name": "NewFilename", "original_string": "func NewFilename(id uint64, dir string) string {\n\treturn filepath.Join(dir, IDToFilename(id))\n}", "language": "go", "code": "func NewFilename(id uint64, dir string) string {\n\treturn filepath.Join(dir, IDToFilename(id))\n}", "code_tokens": ["func", "NewFilename", "(", "id", "uint64", ",", "dir", "string", ")", "string", "{", "return", "filepath", ".", "Join", "(", "dir", ",", "IDToFilename", "(", "id", ")", ")", "\n", "}"], "docstring": "// NewFilename should be named TableFilepath -- it combines the dir with the ID to make a table\n// filepath.", "docstring_tokens": ["NewFilename", "should", "be", "named", "TableFilepath", "--", "it", "combines", "the", "dir", "with", "the", "ID", "to", "make", "a", "table", "filepath", "."], "sha": "6b796b3ebec3ff006fcb1b425836cd784651e9fd", "url": "https://github.com/dgraph-io/badger/blob/6b796b3ebec3ff006fcb1b425836cd784651e9fd/table/table.go#L337-L339", "partition": "test"}
