# P-CodeSum

## Introduction

We introduce P-CodeSum, a novel approach that focuses on automatically generating code summaries tailored to specific projects in few-shot scenarios.

The contributions of this work are as follows:

* We empirically study the performance of large language models (LLMs) on project-specific code summarization tasks and explore key factors to high-quality prompts.

* We propose a neural prompt selector trained on data generated by LLM, obtaining project-specific examples for in-context learning in the few-shot situation.

* We conduct comprehensive experiments to evaluate P-CodeSum. Results demonstrate the high quality of prompts generated by our selector and the significant improvements that P-CodeSum brings to PCS tasks.

The training data and its generation method are explained in [Data](#my-data).

## Data
[data]:#my-data

Training the project-specific prompt selector necessitates the collection of positive and negative data samples. Recognizing the expensive time and effort associated with manual annotation of training data, we employ an LLM to automatically craft high-quality training data.

The code for generating training data and the generated training data are located in the folders 'code' and 'data', respectively.

## Result